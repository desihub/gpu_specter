{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is a notebook to summarize the important lessons we learned at the March 2020 GPU hackathon\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pinned memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pinned memory is good for the situation where we are moving the same data from DtH many times. Preallocating a cpu buffer (pinning memory) means that the data transfer is much faster that it would be with pageable memory. In our case, this best applies to the end of each patch where we move data back to the host. This only makes sense however if we will be using the buffer many times. The overhead of creating pinned memory is expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to pin memory in cupy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preallocate first\n",
    "nwavestep = args.nwavestep\n",
    "flux_out = np.empty((25,nwavestep))\n",
    "ivar_out = np.empty((25,nwavestep))\n",
    "Rdata_out = np.empty((25*nwavestep,25*nwavestep)) #hardcode for now\n",
    "xflux_out = np.empty((25,nwavestep))\n",
    "A_out = np.empty((50000,25*nwavestep))\n",
    "iCov_out = np.empty((25*nwavestep,25*nwavestep))\n",
    "\n",
    "#then pin\n",
    "flux_pinned = _pin_memory(flux_out)\n",
    "ivar_pinned = _pin_memory(ivar_out)\n",
    "Rdata_pinned = _pin_memory(Rdata_out)\n",
    "xflux_pinned = _pin_memory(xflux_out)\n",
    "A_pinned = _pin_memory(A_out)\n",
    "iCov_pinned = _pin_memory(iCov_out)\n",
    "\n",
    "#append all pointers into a list\n",
    "pinned_list = np.array([flux_pinned, ivar_pinned, Rdata_pinned, xflux_pinned, A_pinned, iCov_pinned])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pass pointers to wherever they are needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    results = ex2d(img.pix, img.ivar*(img.mask==0), psfdata, pinned_list, bspecmin[b],\n",
    "        bnspec[b], wave, regularize=args.regularize, ndecorr=args.decorrelate_fibers,\n",
    "        bundlesize=bundlesize, wavesize=args.nwavestep, verbose=args.verbose,\n",
    "        full_output=True, nsubbundles=args.nsubbundles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "put memory in pinned buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fx_gpu_padded = cp.zeros((nspec,nwavesize))\n",
    "fx_gpu_padded[0:nspec,0:nwave] = fx_gpu\n",
    "fx_gpu_padded.get(out=pinned_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "move data back to host in pinned buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = dict(flux=pinned_list[0], ivar=pinned_list[1], R=pinned_list[2], xflux=pinned_list[3], A=pinned_list[4], iCov=pinned_list[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using MPS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MPS is a special NVIDIA MPI helper (?) that condenses GPU operations from multiple MPI ranks in an intelligent way. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We opted not to explore streams because we thought we'd get equally good benefits from MPS with less work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using MPS on corigpu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "if [ $SLURM_PROCID -eq 0 ]; then\n",
    "    nvidia-cuda-mps-control -d\n",
    "fi\n",
    "\n",
    "sleep 10\n",
    "\n",
    "python -u gpu_wrapper_specter.py -o out.fits --nwavestep 75 --nspec 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then run the code inside this mps wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this was an ugly workaround -- it may be possible to run without this now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using multiple GPUs via MPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    cp.cuda.Device(rank).use()\n",
    "    print(\"moving work to %s\" %(rank))\n",
    "except Exception as e:\n",
    "    #print(\"e\", e)\n",
    "    print(\"only 1 gpu, will continue on Device 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profiling the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nvprof"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nvprof is now no longer reccomended-- nvidia officially supports nsight systems/compute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the best reason to use this tool is to profile mpi jobs since it can display data from several ranks stacked"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "srun nvprof --log-file desi_nvprof_02252020.log python -u gpu_wrapper_specter.py -o test.fits --nspec 50 --nwavestep 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nsight systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On cori gpu run nsys and write .qdrep file, move to laptop for local analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srun nsys profile -s none -o desi_nsys_02252020 -t cuda,nvtx --force-overwrite true --stats=true python -u gpu_wrapper_specter.py -o test.fits --nspec 50 --nwavestep 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nsight compute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is really slow and I never actually ran it to completion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the kernel name -k is what the compiler calls the kernel. You see this by looking in nsys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time srun nv-nsight-cu-cli -k dlaed4 -o desi_ncom_02282020 -f python -u gpu_wrapper_specter.py -o out.fits --nspec 50 --nwavestep 50\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Haswell cpu time to beat"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Baseline data taken on Cori Haswell using $SCRATCH at 4:30PM Friday, Feb 28."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "module load python\n",
    "source /global/cfs/cdirs/desi/software/desi_environment.sh master\n",
    "time srun -n 20 -c 2 -u python -u cpu_wrapper_specter.py -o out.fits"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1m49.842s\n",
    "1m46.758s\n",
    "1m49.340s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
